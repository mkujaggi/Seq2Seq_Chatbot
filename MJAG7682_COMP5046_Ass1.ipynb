{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MJAG7682_COMP5046_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkujaggi/Seq2Seq_Chatbot/blob/master/MJAG7682_COMP5046_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "To run the chat with pre trained model kindly run the following sections:\n",
        "1.1, 1.2, 2.1, 2.1.6, 2.2.1, 2.2.2,  2.2.3, 2.2.4, 2.2.5, 3.1, 3.2, 3.3, 3.4, 3.5.2\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "1fd5f8ab-75ed-4a01-cfae-17d7b90b128c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "id = '1ZPLdx8KBBQVSEo760SQ0hoQ85hlN_0Cu'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')  \n",
        "id2 = '1NfALGud5wBmunQ61Km9B3fbYw40VJra5'\n",
        "downloaded = drive.CreateFile({'id':id2}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')  \n",
        "id3 = '1nO7DPjhSdRKkptUZNU6dDGOGZhmcFzhy'\n",
        "downloaded = drive.CreateFile({'id':id3}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 10.7MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "Using Tweet tokenizer as it does not seperate words with apostrophe and converting the words to lower case as it will help predicting the word.*\n",
        "*Using lemmatizing because by converting the word to its root form gives result which are in the language and chatbot responses will be more efficient. Also removing the stop words as it will reduce the time in lemmatizing and and stop words are not required in the chat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetching chat personality data\n",
        "df_professional = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
        "df_comic = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n",
        "df_friend = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "# Class for preprocessing data\n",
        "class ChatPersonality:\n",
        "  def preprocessData(self, df):\n",
        "    unique_words=[]\n",
        "    whole_data = []\n",
        "    chat_data = []\n",
        "    max_input_word = 0\n",
        "    for index, row in df.iterrows():\n",
        "      question = row.Question\n",
        "      answer = row.Answer\n",
        "      chat_data.append([question,answer])\n",
        "      question = re.sub(r'[^\\w\\s]','',question) #Removing Special characters\n",
        "      question = question.lower()\n",
        "      q_token = tknzr.tokenize(question) #Tweet Tokenizing the data\n",
        "      a_token = [answer]\n",
        "      q_lemmatized = []\n",
        "      for w in q_token:\n",
        "        q_lemmatized.append(wordnet_lemmatizer.lemmatize(w)) # Lemmatizing data\n",
        "      whole_data += q_lemmatized\n",
        "      whole_data += a_token\n",
        "      max_input_word = max(len(q_lemmatized), max_input_word)\n",
        "    unique_words = list(set(whole_data))\n",
        "    unique_words.append('_B_')\n",
        "    unique_words.append('_E_')\n",
        "    unique_words.append('_P_')\n",
        "    unique_words.append('_U_')\n",
        "    num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "    dic_len = len(num_dic)\n",
        "    return num_dic, max_input_word, chat_data, unique_words\n",
        "obj = ChatPersonality()\n",
        "\n",
        "dic_professional, max_input_professional, chat_professional, unique_words_pro = obj.preprocessData(df_professional)\n",
        "dic_comic, max_input_comic, chat_comic, unique_words_com = obj.preprocessData(df_comic)\n",
        "dic_friend, max_input_friend, chat_friend, unique_words_fri = obj.preprocessData(df_friend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "Using  fastText as it will be able to predict more number of words than in the dataset. Using CBOW as it will see the context of the one word and not give out multiple words as in as in Skip-gram, also since we are dealing with large data set using CBOW would be a better approach. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab_type": "code",
        "outputId": "8c04403d-108c-4dd9-effe-e9eec1c8111b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install paramiko\n",
        "from gensim.models import FastText"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting paramiko\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/ae/94e70d49044ccc234bfdba20114fa947d7ba6eb68a2e452d89b920e62227/paramiko-2.4.2-py2.py3-none-any.whl (193kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 6.7MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1 (from paramiko)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/15/2cd0a203f318c2240b42cd9dd13c931ddd61067809fee3479f44f086103e/PyNaCl-1.3.0-cp34-abi3-manylinux1_x86_64.whl (759kB)\n",
            "\u001b[K    100% |████████████████████████████████| 768kB 24.9MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3 (from paramiko)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/79/79a4d167a31cc206117d9b396926615fa9c1fdbd52017bcced80937ac501/bcrypt-3.1.6-cp34-abi3-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.5 (from paramiko)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from paramiko) (0.4.5)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.0)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=1.5->paramiko)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko) (2.19)\n",
            "Installing collected packages: pynacl, bcrypt, asn1crypto, cryptography, paramiko\n",
            "Successfully installed asn1crypto-0.24.0 bcrypt-3.1.6 cryptography-2.6.1 paramiko-2.4.2 pynacl-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "source": [
        "*Choosing movie lines dataset because it contains large vocabolary for model to better train and also, since it contains movie lines it would help train the model with regular slangs as well. *\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLjf_pm9NiA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Downloading Movie lines dataset from Google drive\n",
        "id4 = '1bTKKJemxya3QHSCGJWuyf3UeP5UGxnmg'\n",
        "downloaded = drive.CreateFile({'id':id4}) \n",
        "downloaded.GetContentFile('movie_lines.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        "Using Tweet tokenizer as it does not seperate words with apostrophe and converting the words to lower case as it will help predicting the word. Not removing stop words as it will loose the context to certain words and using lemmatizing because by converting the word to its root form gives result which are in the language and chatbot responses will be more efficient. Also removing the  special characters to reduce the vocabolary and clean the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing data for Word Embeddings\n",
        "df_movie_lines = []\n",
        "df_movie = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "for line in df_movie:\n",
        "  #extracting dialogues from data file\n",
        "  lines = line.split(' +++$+++ ')\n",
        "  m_line = re.sub(r'[^\\w\\s]','',lines[-1])\n",
        "  m_line = m_line.lower()\n",
        "  # Tweet tokenizing the data\n",
        "  m_line_token = tknzr.tokenize(m_line)\n",
        "  m_lemma = []\n",
        "  for line in m_line_token:\n",
        "    m_lemma.append(wordnet_lemmatizer.lemmatize(line))\n",
        "  df_movie_lines.append(m_lemma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "Choosing window size as 3 since dataset contains small conversations, so taking a small window size will help finding the context of the word, as it will take 3 words from either side of the center word. Also taking size of 100  at a time.  Taking Min_count as five to remove the rare words from the vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "ft_cbow_model = FastText(size=100, window=3, min_count=5, workers=4, sg=0)\n",
        "ft_cbow_model.build_vocab(sentences=df_movie_lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training word Embeddings and Printing the Loss\n",
        "\n",
        "ft_cbow_model.train(sentences=df_movie_lines, total_examples=len(df_movie_lines), epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "outputId": "13dd0041-d01d-44c1-8351-c8e3f4ad392c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Saving word embeddings in the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "ft_cbow_model.save('/content/gdrive/My Drive/assignment1/movie_dialog_title.model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "outputId": "0b8758c1-f326-4d50-cd33-9cb1bd0674e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Retriving world embeddings from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "cow_model = FastText.load(\"/content/gdrive/My Drive/assignment1/movie_dialog_title.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting answers data to vectors format\n",
        "def get_vectors_a(sentence, dic_personality):    \n",
        "    tokenized_sentence = [sentence]\n",
        "    data = tokens_to_ids(tokenized_sentence, dic_personality)\n",
        "    return data\n",
        "\n",
        "def tokens_to_ids(tokenized_sentence, dic_personality):\n",
        "    ids = []\n",
        "    for token in tokenized_sentence:\n",
        "        if token in dic_personality:\n",
        "            ids.append(dic_personality[token])\n",
        "        else:\n",
        "            ids.append(dic_personality['_U_'])   \n",
        "\n",
        "    return ids\n",
        "# Method for converting Personality dataset to vectors for output of Tensorflow \n",
        "def make_batch(seq_data, dic_personality):\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:              \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "        output_data = [dic_personality['_B_']]\n",
        "        output_data += get_vectors_a(seq[1], dic_personality)\n",
        "        # Output of decoder cell, Add '_E_' at the end of the sequence data\n",
        "        target = get_vectors_a(seq[1], dic_personality)\n",
        "        target.append(dic_personality['_E_'])\n",
        "        \n",
        "        # Convert each token vector to one-hot encode data\n",
        "        output_batch.append(np.eye(len(dic_personality))[output_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "    \n",
        "    return output_batch, target_batch\n",
        "# Method for creating vectors from WordEmbeddings for input in Tensorflow\n",
        "def get_input_vectors(model):\n",
        "  id_to_token = ['_U_']\n",
        "  token_to_id = {'_U_': 0}\n",
        "  for tokens in model.wv.vocab:\n",
        "    if tokens not in token_to_id:\n",
        "        token_to_id[tokens] = len(token_to_id)\n",
        "        id_to_token.append(tokens)\n",
        "  vector_list = []\n",
        "  scale = np.sqrt(3.0 / 100)\n",
        "  for word in id_to_token:\n",
        "    if word in model.wv.vocab:\n",
        "      vector_list.append(np.array(model.wv[word]))\n",
        "    else:\n",
        "      random_vector = np.random.uniform(-scale, scale, [100])\n",
        "      vector_list.append(random_vector)\n",
        "  return vector_list\n",
        "vector_list = get_input_vectors(cow_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "source": [
        "Choosing a learning rate and epoch that doe not take a lot of time and does not cause instability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "\n",
        "\n",
        "def encoder():\n",
        "  tf.reset_default_graph()\n",
        "  enc_input = tf.placeholder(tf.float32, shape=[None,None,100])\n",
        "   # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  return enc_input\n",
        "\n",
        "def decoder(dic_personality):\n",
        "  n_class = len(dic_personality)\n",
        "  n_input = len(dic_personality)\n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "  \n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              dtype=tf.float32)\n",
        "  \n",
        "  model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "  cost = tf.reduce_mean(\n",
        "              tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                  logits=model, labels=targets))\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "  return dec_input, targets, cost, optimizer, model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def trainModel(optimizer, cost, enc_input, dec_input, targets, vector_list, output_batch, target_batch):\n",
        "  total_epoch = 5000\n",
        "  for epoch in range(total_epoch):\n",
        "      _, loss = sess.run([optimizer, cost],\n",
        "                         feed_dict={enc_input: [vector_list],\n",
        "                                    dec_input: output_batch,\n",
        "                                    targets: target_batch})\n",
        "      if epoch % 100 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1),\n",
        "                'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "  print('Epoch:', '%04d' % (epoch + 1),\n",
        "        'cost =', '{:.6f}'.format(loss))\n",
        "  print('Training completed')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "outputId": "8aea22c3-765a-435e-9de7-52b4d9a48ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Please comment your code\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "def train_and_save(personality):\n",
        "  \n",
        "  saver = tf.train.Saver()\n",
        "  save_path = saver.save(sess, \"/content/gdrive/My Drive/assignment1/SS_model_\"+personality+\".ckpt\")\n",
        "  print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading trained data from google drive\n",
        "def loadModel(personality):\n",
        "  pro = tf.Graph()\n",
        "  personality = personality.lower()\n",
        "  sess = tf.Session(graph = pro)\n",
        "  with pro.as_default():\n",
        "    saver = tf.train.import_meta_graph(\"/content/gdrive/My Drive/assignment1/SS_model_\"+personality+\".ckpt.meta\")\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    saver.restore(sess, \"/content/gdrive/My Drive/assignment1/SS_model_\"+personality+\".ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_personality = dic_professional\n",
        "person=\"professional\"\n",
        "max_input_personality = max_input_professional\n",
        "unique_words = unique_words_pro\n",
        "def answer(sentence):\n",
        "\n",
        "  global count\n",
        "  global max_input_personality\n",
        "  global dic_personality\n",
        "  global person\n",
        "  global unique_words\n",
        "  if sentence[:7] ==\"*change\":\n",
        "    dic_personality, max_input_personality, tmp_person, unique_words = changePersonality(sentence, True)      \n",
        "    if max_input_personality == 0:\n",
        "      dic_personality, max_input_personality, person, unique_words = changePersonality(\"*change: \"+person, True)\n",
        "      return \"Personality not found. Kindly choose from friend, comic or professional.\"\n",
        "    elif tmp_person == \"friend\":\n",
        "      person = tmp_person\n",
        "      enc_input = encoder()\n",
        "      dec_input, targets, cost, optimizer, model = decoder(dic_personality)\n",
        "      return \"Now I will talk like a friend.\"\n",
        "    elif tmp_person == \"comic\":\n",
        "      person = tmp_person\n",
        "      enc_input = encoder()\n",
        "      dec_input, targets, cost, optimizer, model = decoder(dic_personality)\n",
        "      return \"Now I will talk like a comic\"\n",
        "    elif tmp_person == \"professional\":\n",
        "      person = tmp_person\n",
        "      enc_input = encoder()\n",
        "      dec_input, targets, cost, optimizer, model = decoder(dic_personality)\n",
        "      return \"Now I will talk like a professional.\"\n",
        "  elif  sentence[:4].lower() == \"quit\":\n",
        "    return -1\n",
        "  else:\n",
        "    enc_input = encoder()\n",
        "    dec_input, targets, cost, optimizer, model = decoder(dic_personality)\n",
        "  seq_data = [sentence, '_U_' * 1]\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  output_batch, target_batch = make_batch([seq_data], dic_personality)\n",
        "  prediction = tf.argmax(model, 2)\n",
        "  result = sess.run(prediction,\n",
        "                    feed_dict={enc_input: [vector_list],\n",
        "                               dec_input: output_batch,\n",
        "                               targets: target_batch})\n",
        "\n",
        "  # convert index number to actual token \n",
        "  decoded = [unique_words[i] for i in result[0]]\n",
        "  # Remove anything after '_E_'        \n",
        "  if \"_E_\" in decoded:\n",
        "      end = decoded.index('_E_')\n",
        "      translated = ' '.join(decoded[:end])\n",
        "  elif \"_B_\" in decoded:\n",
        "    start = decoded.index('_B_')\n",
        "    translated = ' '.join(decoded[start:])\n",
        "  else :\n",
        "      translated = ' '.join(decoded[:])\n",
        "\n",
        "  return translated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "source": [
        "*To change the personality in the chat type \"*change: personality\" \n",
        "Example type \"\\*change: comic\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def changePersonality(sentence, isChange):\n",
        "  if isChange:\n",
        "    dic_personality = {}\n",
        "    max_input_personality = 0\n",
        "    unique_words = []\n",
        "    personality = re.split(r':', sentence)\n",
        "    person = personality[-1].strip()\n",
        "    if person.lower() == \"comic\":\n",
        "      dic_personality = dic_comic\n",
        "      max_input_personality = max_input_comic\n",
        "      unique_words = unique_words_com\n",
        "      loadModel(person)\n",
        "    elif person.lower() == \"friend\":\n",
        "      dic_personality = dic_friend\n",
        "      max_input_personality = max_input_friend\n",
        "      unique_words = unique_words_fri\n",
        "      loadModel(person)\n",
        "    elif person.lower() == \"professional\":\n",
        "      dic_personality = dic_professional\n",
        "      max_input_personality = max_input_professional\n",
        "      unique_words = unique_words_pro\n",
        "      loadModel(person)\n",
        "    else:\n",
        "      return dic_personality, max_input_personality, person.lower(), unique_words\n",
        "  else:\n",
        "    dic_personality = dic_professional\n",
        "    max_input_personality = max_input_professional\n",
        "    unique_words = unique_words_pro\n",
        "    person = \"professional\"\n",
        "  return dic_personality, max_input_personality, person.lower(), unique_words\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def save_chat_log(chat_log):\n",
        "  chatLogFile = open('/content/gdrive/My Drive/assignment1/MJAG7682_chat_log.txt', 'a')\n",
        "  for chat in chat_log:\n",
        "    chatLogFile.write(\"\\n\"+str(chat))\n",
        "  chatLogFile.write('\\n'+'---------------------------------------------------')\n",
        "  chatLogFile.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5TfzP58TbZJ",
        "colab_type": "text"
      },
      "source": [
        "To end chat just type \"quit\" in the chat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT_DeoHSMw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def end_chatting(chat_log):\n",
        "  save_chat_log(chat_log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_woLwuU3Mk3w",
        "colab_type": "text"
      },
      "source": [
        "*Please include lines to train the bot.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhWYz7NQOfLV",
        "colab_type": "code",
        "outputId": "1fd68655-cdf8-4a6f-afe6-03f7fa94fed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2720
        }
      },
      "source": [
        "output_batch, target_batch = make_batch(chat_comic ,dic_comic)\n",
        "enc_input= encoder()\n",
        "dec_input, targets, cost, optimizer, model = decoder(dic_comic)\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "trainModel(optimizer, cost, enc_input, dec_input, targets, vector_list, output_batch, target_batch)\n",
        "train_and_save('comic')\n",
        "\n",
        "\n",
        "enc_input= encoder()\n",
        "output_batch, target_batch = make_batch(chat_professional ,dic_professional)\n",
        "\n",
        "dec_input, targets, cost, optimizer, model = decoder(dic_professional)\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "trainModel(optimizer, cost, enc_input, dec_input, targets, vector_list, output_batch, target_batch)\n",
        "train_and_save('professional')\n",
        "\n",
        "output_batch, target_batch = make_batch(chat_friend ,dic_friend)\n",
        "enc_input= encoder()\n",
        "dec_input, targets, cost, optimizer, model = decoder(dic_friend)\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "trainModel(optimizer, cost, enc_input, dec_input, targets, vector_list, output_batch, target_batch)\n",
        "\n",
        "train_and_save('friend')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 6.411785\n",
            "Epoch: 0101 cost = 2.243492\n",
            "Epoch: 0201 cost = 2.247497\n",
            "Epoch: 0301 cost = 2.247127\n",
            "Epoch: 0401 cost = 2.233783\n",
            "Epoch: 0501 cost = 2.237411\n",
            "Epoch: 0601 cost = 2.234044\n",
            "Epoch: 0701 cost = 2.239955\n",
            "Epoch: 0801 cost = 2.240420\n",
            "Epoch: 0901 cost = 2.238179\n",
            "Epoch: 1001 cost = 2.243143\n",
            "Epoch: 1101 cost = 2.243031\n",
            "Epoch: 1201 cost = 2.234694\n",
            "Epoch: 1301 cost = 2.236718\n",
            "Epoch: 1401 cost = 2.233294\n",
            "Epoch: 1501 cost = 2.239013\n",
            "Epoch: 1601 cost = 2.239004\n",
            "Epoch: 1701 cost = 2.235436\n",
            "Epoch: 1801 cost = 2.236678\n",
            "Epoch: 1901 cost = 2.236172\n",
            "Epoch: 2001 cost = 2.234873\n",
            "Epoch: 2101 cost = 2.239042\n",
            "Epoch: 2201 cost = 2.236275\n",
            "Epoch: 2301 cost = 2.234096\n",
            "Epoch: 2401 cost = 2.238358\n",
            "Epoch: 2501 cost = 2.236131\n",
            "Epoch: 2601 cost = 2.232791\n",
            "Epoch: 2701 cost = 2.239544\n",
            "Epoch: 2801 cost = 2.236048\n",
            "Epoch: 2901 cost = 2.236382\n",
            "Epoch: 3001 cost = 2.232912\n",
            "Epoch: 3101 cost = 2.231715\n",
            "Epoch: 3201 cost = 2.234505\n",
            "Epoch: 3301 cost = 2.235190\n",
            "Epoch: 3401 cost = 2.229778\n",
            "Epoch: 3501 cost = 2.233973\n",
            "Epoch: 3601 cost = 2.235149\n",
            "Epoch: 3701 cost = 2.233896\n",
            "Epoch: 3801 cost = 2.232860\n",
            "Epoch: 3901 cost = 2.242054\n",
            "Epoch: 4001 cost = 2.235331\n",
            "Epoch: 4101 cost = 2.237048\n",
            "Epoch: 4201 cost = 2.235116\n",
            "Epoch: 4301 cost = 2.238672\n",
            "Epoch: 4401 cost = 2.233161\n",
            "Epoch: 4501 cost = 2.234757\n",
            "Epoch: 4601 cost = 2.235409\n",
            "Epoch: 4701 cost = 2.236035\n",
            "Epoch: 4801 cost = 2.236087\n",
            "Epoch: 4901 cost = 2.235845\n",
            "Epoch: 5000 cost = 2.235760\n",
            "Training completed\n",
            "Model saved in path: /content/gdrive/My Drive/assignment1/SS_model_comic.ckpt\n",
            "Epoch: 0001 cost = 6.398056\n",
            "Epoch: 0101 cost = 2.252527\n",
            "Epoch: 0201 cost = 2.243247\n",
            "Epoch: 0301 cost = 2.243612\n",
            "Epoch: 0401 cost = 2.242306\n",
            "Epoch: 0501 cost = 2.240308\n",
            "Epoch: 0601 cost = 2.242072\n",
            "Epoch: 0701 cost = 2.238053\n",
            "Epoch: 0801 cost = 2.235550\n",
            "Epoch: 0901 cost = 2.235300\n",
            "Epoch: 1001 cost = 2.238163\n",
            "Epoch: 1101 cost = 2.236804\n",
            "Epoch: 1201 cost = 2.240720\n",
            "Epoch: 1301 cost = 2.237864\n",
            "Epoch: 1401 cost = 2.232492\n",
            "Epoch: 1501 cost = 2.233546\n",
            "Epoch: 1601 cost = 2.236321\n",
            "Epoch: 1701 cost = 2.237365\n",
            "Epoch: 1801 cost = 2.231405\n",
            "Epoch: 1901 cost = 2.234426\n",
            "Epoch: 2001 cost = 2.237000\n",
            "Epoch: 2101 cost = 2.236532\n",
            "Epoch: 2201 cost = 2.240364\n",
            "Epoch: 2301 cost = 2.233832\n",
            "Epoch: 2401 cost = 2.235526\n",
            "Epoch: 2501 cost = 2.234361\n",
            "Epoch: 2601 cost = 2.236549\n",
            "Epoch: 2701 cost = 2.231943\n",
            "Epoch: 2801 cost = 2.234951\n",
            "Epoch: 2901 cost = 2.235712\n",
            "Epoch: 3001 cost = 2.234639\n",
            "Epoch: 3101 cost = 2.235542\n",
            "Epoch: 3201 cost = 2.236080\n",
            "Epoch: 3301 cost = 2.235191\n",
            "Epoch: 3401 cost = 2.234368\n",
            "Epoch: 3501 cost = 2.230932\n",
            "Epoch: 3601 cost = 2.239229\n",
            "Epoch: 3701 cost = 2.235181\n",
            "Epoch: 3801 cost = 2.234072\n",
            "Epoch: 3901 cost = 2.232100\n",
            "Epoch: 4001 cost = 2.235321\n",
            "Epoch: 4101 cost = 2.237365\n",
            "Epoch: 4201 cost = 2.236187\n",
            "Epoch: 4301 cost = 2.236093\n",
            "Epoch: 4401 cost = 2.235062\n",
            "Epoch: 4501 cost = 2.234727\n",
            "Epoch: 4601 cost = 2.234600\n",
            "Epoch: 4701 cost = 2.238929\n",
            "Epoch: 4801 cost = 2.238958\n",
            "Epoch: 4901 cost = 2.237242\n",
            "Epoch: 5000 cost = 2.236044\n",
            "Training completed\n",
            "Model saved in path: /content/gdrive/My Drive/assignment1/SS_model_professional.ckpt\n",
            "Epoch: 0001 cost = 6.405047\n",
            "Epoch: 0101 cost = 2.248948\n",
            "Epoch: 0201 cost = 2.247554\n",
            "Epoch: 0301 cost = 2.250790\n",
            "Epoch: 0401 cost = 2.248918\n",
            "Epoch: 0501 cost = 2.248409\n",
            "Epoch: 0601 cost = 2.246494\n",
            "Epoch: 0701 cost = 2.249781\n",
            "Epoch: 0801 cost = 2.245826\n",
            "Epoch: 0901 cost = 2.241773\n",
            "Epoch: 1001 cost = 2.245224\n",
            "Epoch: 1101 cost = 2.245014\n",
            "Epoch: 1201 cost = 2.243197\n",
            "Epoch: 1301 cost = 2.243173\n",
            "Epoch: 1401 cost = 2.245291\n",
            "Epoch: 1501 cost = 2.239745\n",
            "Epoch: 1601 cost = 2.243132\n",
            "Epoch: 1701 cost = 2.242406\n",
            "Epoch: 1801 cost = 2.243757\n",
            "Epoch: 1901 cost = 2.243392\n",
            "Epoch: 2001 cost = 2.240485\n",
            "Epoch: 2101 cost = 2.240378\n",
            "Epoch: 2201 cost = 2.239054\n",
            "Epoch: 2301 cost = 2.242255\n",
            "Epoch: 2401 cost = 2.242075\n",
            "Epoch: 2501 cost = 2.240039\n",
            "Epoch: 2601 cost = 2.239886\n",
            "Epoch: 2701 cost = 2.243296\n",
            "Epoch: 2801 cost = 2.238259\n",
            "Epoch: 2901 cost = 2.240700\n",
            "Epoch: 3001 cost = 2.242725\n",
            "Epoch: 3101 cost = 2.245554\n",
            "Epoch: 3201 cost = 2.242754\n",
            "Epoch: 3301 cost = 2.244988\n",
            "Epoch: 3401 cost = 2.239131\n",
            "Epoch: 3501 cost = 2.240010\n",
            "Epoch: 3601 cost = 2.242688\n",
            "Epoch: 3701 cost = 2.241990\n",
            "Epoch: 3801 cost = 2.240862\n",
            "Epoch: 3901 cost = 2.237686\n",
            "Epoch: 4001 cost = 2.242248\n",
            "Epoch: 4101 cost = 2.242629\n",
            "Epoch: 4201 cost = 2.243030\n",
            "Epoch: 4301 cost = 2.240722\n",
            "Epoch: 4401 cost = 2.243224\n",
            "Epoch: 4501 cost = 2.246365\n",
            "Epoch: 4601 cost = 2.239812\n",
            "Epoch: 4701 cost = 2.241958\n",
            "Epoch: 4801 cost = 2.239765\n",
            "Epoch: 4901 cost = 2.242438\n",
            "Epoch: 5000 cost = 2.236209\n",
            "Training completed\n",
            "Model saved in path: /content/gdrive/My Drive/assignment1/SS_model_friend.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "source": [
        "*Please include lines to start chatting with the bot.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "outputId": "646da8e5-2fe5-453b-c60e-1e084df8b722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2346
        }
      },
      "source": [
        "# Please comment your code\n",
        "loadModel(\"professional\")\n",
        "def chatBot():\n",
        "  Continue = 1\n",
        "  chatlog = []\n",
        "  \n",
        "  print(\"Bot: Hello\")\n",
        "  count = 0\n",
        "  chatlog.append(\"Bot: Hello\")\n",
        "  while Continue == 1:\n",
        "    \n",
        "    ques = input(\"You: \")\n",
        "    \n",
        "    result = answer(ques)\n",
        "    count = count+1\n",
        "    chatlog.append(\"You: \"+ques)\n",
        "    if result == -1:\n",
        "      print(\"Bot: Chat saved.See you later.\")\n",
        "      chatlog.append(\"Bot: Chat saved.See you later.\")\n",
        "      end_chatting(chatlog)\n",
        "      Continue = 0\n",
        "    else:\n",
        "      print(\"Bot: \"+result)\n",
        "      chatlog.append(\"Bot: \"+result)\n",
        "chatBot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/assignment1/SS_model_professional.ckpt\n",
            "Bot: Hello\n",
            "You: HI\n",
            "Bot: worst alligator\n",
            "You: yes aligators are the worst\n",
            "Bot: google job\n",
            "You: you can also do a google job\n",
            "Bot: season interest\n",
            "You: So how are you?\n",
            "Bot: ask get\n",
            "You: What is your favorite food?\n",
            "Bot: greeting I don't need to eat.\n",
            "You: but why?\n",
            "Bot: I'm here to answer your questions and help out. mad\n",
            "You: So you must get hungry while answering so many questions\n",
            "Bot: feel afraid\n",
            "You: Don't be afraid\n",
            "Bot: tomorrow your\n",
            "You: ok cool\n",
            "Bot: guy tall\n",
            "You: Yes a guy can be tall\n",
            "Bot: Very well. why\n",
            "You: ok are you tall?\n",
            "Bot: yup I enjoy talking with you.\n",
            "You: Thank you. I also enjoy talking to you\n",
            "Bot: not whered\n",
            "You: ok\n",
            "Bot: getting and\n",
            "You: greetings\n",
            "Bot: thanks sibling\n",
            "You: your welcome\n",
            "Bot: afraid favorite\n",
            "You: time so say good bye\n",
            "Bot: at yesterday\n",
            "You: *change: comic\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/assignment1/SS_model_comic.ckpt\n",
            "Bot: Now I will talk like a comic\n",
            "You: ok great that could be fun\n",
            "Bot: beautiful into\n",
            "You: into what\n",
            "Bot: ticked prettier\n",
            "You: ok great\n",
            "Bot: old chat\n",
            "You: yeah old chat\n",
            "Bot: country valentine\n",
            "You: yes\n",
            "Bot: fired run\n",
            "You: what is your age\n",
            "Bot: high change\n",
            "You: what do you eat\n",
            "Bot: I'm sorry to hear that. We can keep chatting if that will help. sing\n",
            "You: yes we can\n",
            "Bot: seem dirty\n",
            "You: no you seem dirty\n",
            "Bot: red didnt\n",
            "You: what is your age\n",
            "Bot: suck That's a bummer.\n",
            "You: ok great\n",
            "Bot: starving assistant\n",
            "You: are you hungry\n",
            "Bot: alexa and\n",
            "You: google assistant\n",
            "Bot: science have\n",
            "You: science have what\n",
            "Bot: people Sometimes I like to take a break from being awesome.\n",
            "You: I can never take a break from being awesome\n",
            "Bot: now Nah, I'm good.\n",
            "You: ok \n",
            "Bot: family more\n",
            "You: what do you like to talk about\n",
            "Bot: shush Oh.\n",
            "You: hey what are you upto\n",
            "Bot: see I acknowledge your presence.\n",
            "You: I acknowledge it too\n",
            "Bot: are bump\n",
            "You: ok great\n",
            "Bot: such bump\n",
            "You: I am getting bored\n",
            "Bot: till Behold the field in which I grow my jokes and see that it is barren.\n",
            "You: Ok tell me a joke\n",
            "Bot: love you\n",
            "You: love you too\n",
            "Bot: Sure! We should get matching sweaters. No prob.\n",
            "You: Yes we should\n",
            "Bot: cool no\n",
            "You: ok lets talk later\n",
            "Bot: husband You're excused.\n",
            "You: *change: frien\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/assignment1/SS_model_comic.ckpt\n",
            "Bot: Personality not found. Kindly choose from friend, comic or professional.\n",
            "You: *change: friend\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/assignment1/SS_model_friend.ckpt\n",
            "Bot: Now I will talk like a friend.\n",
            "You: ok great\n",
            "Bot: appreciate again\n",
            "You: you are awesome\n",
            "Bot: The only thing I'm committed to is being a great friend. fake\n",
            "You: that is good to hear\n",
            "Bot: something friend\n",
            "You: I am also a great friend\n",
            "Bot: real call\n",
            "You: yes it is real\n",
            "Bot: Nice to meet you too! tune\n",
            "You: Same here\n",
            "Bot: think funny\n",
            "You: What do you eat\n",
            "Bot: would ask\n",
            "You: what is your age\n",
            "Bot: friend whats\n",
            "You: how you doin\n",
            "Bot: cracking later\n",
            "You: what are you doing now\n",
            "Bot: So happy! dating\n",
            "You: who are you dating\n",
            "Bot: girlfriend out\n",
            "You: who is your girlfriend\n",
            "Bot: sound It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\n",
            "You: ok lets keep chatting\n",
            "Bot: bot feel\n",
            "You: yes you are a bot\n",
            "Bot: hum believe\n",
            "You: believe in what\n",
            "Bot: partner animal\n",
            "You: which animal do you like\n",
            "Bot: nighty mother\n",
            "You: that is not a animal\n",
            "Bot: lame Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\n",
            "You: you are becoming funny now\n",
            "Bot: time That's not me, but hello nonetheless!\n",
            "You: the who was it\n",
            "Bot: am I can't see you, but I like you!\n",
            "You: time to say good bye\n",
            "Bot: technology Hey there!\n",
            "You: bye\n",
            "Bot: till valentine\n",
            "You: quit\n",
            "Bot: Chat saved.See you later.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}